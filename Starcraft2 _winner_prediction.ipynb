{"cells":[{"metadata":{},"cell_type":"markdown","source":"# 스타크래프트2 게임 빅데이터와 AI를 활용한 승패 예측 알고리즘 개발\n\n\n## 개요\n\n- Blizzard 스타크래프트2의 유저 행동 데이터로 게임 경기의 승패를 예측합니다.\n\n\n## 주최/주관\n\n- 주최 : (사)한국인공지능협회\n\n- 주관 : DACON\n\n- 참조 : https://dacon.io/competitions/official/235583/overview/\n\n\n## 데이터 분석\n#### Table of contents\n* [Loading Data](#1) \n* [Initial Exploration](#2) \n* [Data Cleansing](#3) \n* [Extra data for Feature engineering](#4) \n* [Feature engineering](#5) \n* [Data Preparation (Data Cleansing + Feature Engineering)](#6)\n* [Data Partitioning](#7)\n* [Modeling - light Gradient Boosting Machine (LGBM)](#8)\n* [Feature Importance](#9)\n* [Feature selection](#10)\n* [Modeling - Multi Layer Perceptrons (MLP)](#11)\n* [Performance evaluation of MLP](#12)\n* [Submit Test Results](#13)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Standard libraries\nimport numpy as np\nimport random\nimport pandas as pd\nimport time\nimport re\nimport gc \nfrom tqdm import tqdm  \nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Pre-processing\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport cv2 \n\n# Correlation\nimport scipy\nfrom scipy.cluster import hierarchy as hc # dendrogram\n\n# Model\nfrom sklearn.ensemble import RandomForestRegressor\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras.losses import mse, binary_crossentropy\nfrom keras import optimizers, regularizers\nfrom keras.layers import Input, Dense, Lambda\nfrom keras.models import Sequential, Model, load_model \nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom keras.models import Model\n\n# Evaluate\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn import metrics\nfrom sklearn.tree import export_graphviz\nfrom sklearn.metrics import roc_auc_score\nfrom keras.losses import mse, binary_crossentropy\n\npd.set_option('display.float_format', '{:.2f}'.format)\npd.set_option('display.max_columns', 3000)\npd.set_option('display.max_rows', 3000)\npd.set_option('display.max_colwidth', 3000)\n\n# For notebook plotting\n%matplotlib inline\n\nimport warnings                             \nwarnings.filterwarnings(\"ignore\") ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TPU 사용을 위한 초기화\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() / 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() / 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Loading Data <a id=\"1\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 전처리된 데이터셋 불러오기 \ntrain = pd.read_feather(\"/kaggle/input/star2-processed-dataset/processed_train2.ftr\")\ntrain = reduce_mem_usage(train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Initial Exploration <a id=\"2\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.describe().drop('count')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Cleansing <a id=\"3\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 한쪽 선수의 플레이 기록이 없는 경우에 제거\n\ndef check_missing_values(play_time):\n    check = False\n    if (play_time.min() <= 0) or (play_time.max() <= 0): \n        check = True\n        \n    return check","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Extra data for Feature engineering <a id=\"4\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Use unit information in StarCraft 2\n\n# Protoss\ndef P_info():\n    P_dict = {\n    # VESPENE\n    'BuildAssimilator':[75,0,0,'VESPENE'],\n    \n    # SUPPLY\n    'BuildPylon':[100,0,0,'SUPPLY'],\n    \n    # WORKER\n    'TrainProbe':[50,0,1,'WORKER'],\n    \n    # BASE \n    'BuildNexus':[400,0,0,'BASE'],\n    \n    # BUILDING\n    'BuildGateway':[150,0,0,'BUILDING'],       'BuildTemplarArchive':[150,200,0,'BUILDING'],   'BuildDarkShrine':[150,150,0,'BUILDING'], \n    'BuildRoboticsBay':[200,200,0,'BUILDING'], 'BuildRoboticsFacility':[150,100,0,'BUILDING'], 'BuildStargate':[150,150,0,'BUILDING'], \n    'BuildFleetBeacon':[300,200,0,'BUILDING'], 'BuildForge':[150,0,0,'BUILDING'],              'BuildCyberneticsCore':[150,0,0,'BUILDING'], \n    'BuildTwilightCouncil':[150,100,0,'BUILDING'],'BuildTemplarArchive':[150,200,0,'BUILDING'],'BuildDarkShrine':[150,150,0,'BUILDING'], \n    'BuildFleetBeacon':[300,200,0,'BUILDING'], 'BuildRoboticsBay':[150,150,0,'BUILDING'],\n    \n    # DEFENSE\n    'BuildPhotonCannon':[150,0,0,'DEFENSE'], 'BuildShieldBattery':[100,0,0,'DEFENSE'],\n    \n    # ARMY\n    'TrainZealot':[100,0,2,'ARMY'],        'TrainSentry':[50,100,2,'ARMY'],      'TrainStalker':[125,50,2,'ARMY'],  'TrainHighTemplar':[50,150,2,'ARMY'], \n    'TrainDarkTemplar':[125,125,2,'ARMY'], 'TrainImmortal':[250,100,4,'ARMY'],   'TrainColossus':[300,200,6,'ARMY'],'TrainArchon':[0,0,4,'ARMY'],\n    'TrainObserver':[25,75,1,'ARMY'],      'TrainWarpPrism':[200,0,2,'ARMY'],    'TrainPhoenix':[150,100,2,'ARMY'], 'TrainMothershipCore':[100,100,2,'ARMY'],\n    'TrainVoidRay':[250,150,4,'ARMY'],     'TrainOracle':[150,150,3,'ARMY'],     'TrainTempest':[250,175,5,'ARMY'], 'TrainCarrier':[350,250,6,'ARMY'],\n    'TrainInterceptor':[15,0,0,'ARMY'],    'TrainMothership':[400,400,8,'ARMY'], 'TrainAdept':[100,25,2,'ARMY'],    'TrainDisruptor':[150,150,3,'ARMY'],\n    \n    # UPGRADE    \n    'UpgradeGroundWeapons1':[100,100,0,'UPGRADE'], 'UpgradeGroundWeapons2':[150,150,0,'UPGRADE'],'UpgradeGroundWeapons3':[200,200,0,'UPGRADE'], \n    'UpgradeGroundArmor1':[100,100,0,'UPGRADE'],   'UpgradeGroundArmor2':[150,150,0,'UPGRADE'],  'UpgradeGroundArmor3':[200,200,0,'UPGRADE'],\n    'UpgradeShields1':[150,150,0,'UPGRADE'],       'UpgradeShields2':[225,225,0,'UPGRADE'],      'UpgradeShields3':[300,300,0,'UPGRADE'],\n    'UpgradeAirWeapons1':[100,100,0,'UPGRADE'],    'UpgradeAirWeapons2':[175,175,0,'UPGRADE'],   'UpgradeAirWeapons3':[250,250,0,'UPGRADE'],\n    'UpgradeAirArmor1':[150,150,0,'UPGRADE'],      'UpgradeAirArmor2':[225,225,0,'UPGRADE'],     'UpgradeAirArmor3':[300,300,0,'UPGRADE'],\n    'ResearchCharge':[100,100,0,'UPGRADE'],        'ResearchBlink':[100,100,0,'UPGRADE'],        'ResearchResonatingGlaives':[100,100,0,'UPGRADE'], \n    'ResearchPsiStormTech':[200,200,0,'UPGRADE'],  'ResearchGraviticBoosters':[100,100,0,'UPGRADE'], 'ResearchGraviticDrive':[100,100,0,'UPGRADE'],      \n    'ResearchExtendedThermalLance':[150,150,0,'UPGRADE'],  'ResearchAnionPulseCrystals':[150,150,0,'UPGRADE'],    'ResearchFluxVanes':[100,100,0,'UPGRADE']\n    \n    }\n               \n    return P_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Terran\ndef T_info():\n    T_dict = {\n    # VESPENE\n    'BuildRefinery':[75,0,0,'VESPENE'],\n    \n    # SUPPLY\n    'BuildSupplyDepot':[100,0,0,'SUPPLY'],\n   \n    # WORKER\n    'TrainSCV':[50,0,1,'WORKER'], 'TrainMule':[0,0,0,'WORKER'],\n    \n    # BASE\n    'BuildCommandCenter':[400,0,0,'BASE'], 'UpgradeToPlanetaryFortress':[150,150,0,'BASE'], 'UpgradeToOrbitalCommand':[150,0,0,'BASE'],\n    \n    # BUILDING\n    'BuildBarracks':[150,0,0,'BUILDING'],        'BuildFactory':[150,100,0,'BUILDING'],      'BuildGhostAcademy':[150,50,0,'BUILDING'], 'BuildArmory':[150,100,0,'BUILDING'],\n    'BuildStarport':[150,100,0,'BUILDING'],      'BuildFusionCore':[150,150,0,'BUILDING'],   'BuildEngineeringBay':[125,0,0,'BUILDING'], \n    'BuildSensorTower':[125,100,0,'BUILDING'],   'BuildFactoryTechLab':[50,25,0,'BUILDING'], 'BuildFactoryReactor':[50,50,0,'BUILDING'],  \n    'BuildBarracksTechLab':[50,25,0,'BUILDING'], 'BuildBarracksReactor':[50,50,0,'BUILDING'],\n   \n    # DEFENSE\n    'BuildMissileTurret':[100,0,0,'DEFENSE'], 'BuildAutoTurret':[0,0,0,'DEFENSE'], 'BuildPointDefenseDrone':[0,0,0,'DEFENSE'], 'BuildBunker':[100,0,0,'DEFENSE'],\n    \n    # ARMY\n    'TrainMarine':[50,0,1,'ARMY'],   'TrainMarauder':[100,25,2,'ARMY'],       'TrainReaper':[50,50,1,'ARMY'],    'TrainGhost':[150,125,2,'ARMY'], \n    'BuildHellion':[100,0,2,'ARMY'], 'BuildHellbat':[100,0,2,'ARMY'],         'BuildWidowMine':[75,25,2,'ARMY'], 'BuildSiegeTank':[150,125,3,'ARMY'],\n    'BuildThor':[300,200,6,'ARMY'],  'TrainViking':[150,75,2,'ARMY'],         'TrainMedivac':[100,100,2,'ARMY'], 'TrainBanshee':[150,100,3,'ARMY'],\n    'TrainRaven':[100,200,2,'ARMY'], 'TrainBattlecruiser':[400,300,6,'ARMY'], 'TrainCyclone':[150,100,3,'ARMY'], 'TrainLiberator':[150,150,3,'ARMY'],\n    \n    # UPGRADE\n    'ResearchNeosteelArmor':[150,150,0,'UPGRADE'],                    'ResearchNeosteelFrame':[100,100,0,'UPGRADE'],           'ResearchHiSecAutoTracking':[100,100,0,'UPGRADE'],\n    'UpgradeTerranInfantryWeapons1':[100,100,0,'UPGRADE'],            'UpgradeTerranInfantryWeapons2':[175,175,0,'UPGRADE'],   'UpgradeTerranInfantryWeapons3':[250,250,0,'UPGRADE'],\n    'UpgradeTerranInfantryArmor1':[100,100,0,'UPGRADE'],              'UpgradeTerranInfantryArmor2':[175,175,0,'UPGRADE'],     'UpgradeTerranInfantryArmor3':[250,250,0,'UPGRADE'],\n    'UpgradeStructureArmor':[150,150,0,'UPGRADE'],                    'ResearchPersonalCloaking':[150,150,0,'UPGRADE'],        'ResearchEnhancedShockwaves':[150,150,0,'UPGRADE'],\n    'TrainNuke':[100,100,0,'UPGRADE'],                                'ResearchRapidReignitionSystem':[100,100,0,'UPGRADE'],\n    'UpgradeVehicleWeapons1':[100,100,0,'UPGRADE'],                   'UpgradeVehicleWeapons2':[175,175,0,'UPGRADE'],           'UpgradeVehicleWeapons3':[250,250,0,'UPGRADE'],\n    'UpgradeShipWeapons1':[100,100,0,'UPGRADE'],                      'UpgradeShipWeapons2':[175,175,0,'UPGRADE'],              'UpgradeShipWeapons3':[250,250,0,'UPGRADE'],\n    'ResearchTerranVehicleAndShipArmorsLevel1':[100,100,0,'UPGRADE'], 'ResearchTerranVehicleAndShipArmorsLevel2':[175,175,0,'UPGRADE'],\n    'ResearchTerranVehicleAndShipArmorsLevel1':[250,250,0,'UPGRADE'], 'ResearchWeaponRefit':[150,150,0,'UPGRADE'],                 'ResearchAdvancedBallistics':[150,150,0,'UPGRADE'], \n    'ResearchCombatShield':[100,100,0,'UPGRADE'],                     'ResearchStimpack':[100,100,0,'UPGRADE'],                    'ResearchConcussiveShells':[50,50,0,'UPGRADE'],\n    'ResearchInfernalPreIgniter':[100,100,0,'UPGRADE'],               'ResearchMagFieldAccelerator':[100,100,0,'UPGRADE'],         'ResearchDrillingClaws':[75,75,0,'UPGRADE'],\n    'ResearchSmartServos':[100,100,0,'UPGRADE'],                      'ResearchCorvidReactor':[150,150,0,'UPGRADE'],               'ResearchCloakingField':[100,100,0,'UPGRADE'],\n    'ResearchHyperflightRotors':[150,150,0,'UPGRADE'],                'ResearchRavenRecalibratedExplosives':[150,150,0,'UPGRADE'], 'ResearchRapidFireLaunchers':[75,75,0,'UPGRADE']\n    \n    }\n    \n    return T_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Zerg \ndef Z_info():\n    Z_dict = {\n    # VESPENE\n    'BuildExtractor':[25,0,0,'VESPENE'],\n\n    # SUPPLY\n    'MorphOverlord':[100,0,0,'SUPPLY'],\n    \n    # WORKER\n    'MorphDrone':[50,0,1,'WORKER'],\n    \n    # BASE\n    'BuildHatchery':[300,0,0,'BASE'], 'UpgradeToLair':[150,100,0,'BASE'], 'UpgradeToHive':[200,150,0,'BASE'],\n    \n    # BUILDING\n    'BuildSpawningPool':[200,0,0,'BUILDING'],      'BuildRoachWarren':[150,0,0,'BUILDING'],      'BuildBanelingNest':[100,50,0,'BUILDING'], \n    'BuildUltraliskCavern':[150,200,0,'BUILDING'], 'BuildHydraliskDen':[100,100,0,'BUILDING'],   'BuildInfestationPit':[100,100,0,'BUILDING'],\n    'BuildSpire':[200,200,0,'BUILDING'],           'MorphToGreaterSpire':[100,150,0,'BUILDING'], 'UpgradeToLurkerDenMP':[100,150,0,'BUILDING'], 'BuildCreepTumor':[0,0,0,'BUILDING'],\n    'BuildEvolutionChamber':[75,0,0,'BUILDING'],   'BuildNydusNetwork':[150,150,0,'BUILDING'],   'BuildNydusWorm':[50,50,0,'BUILDING'],  \n    \n    # DEFENSE\n    \"BuildSpineCrawler\":[100,0,0,'DEFENSE'], \"BuildSporeCrawler\":[75,0,0,'DEFENSE'],\n    \n    # ARMY\n    'TrainQueen':[150,0,2,'ARMY'],       'MorphZergling':[25,0,0.5,'ARMY'],  'TrainBaneling':[25,25,0.5,'ARMY'],    'MorphRoach':[75,25,2,'ARMY'], \n    'MorphHydralisk':[100,50,2,'ARMY'],  'MorphInfestor':[100,150,2,'ARMY'], 'MorphSwarmHost':[100,75,3,'ARMY'], \n    'MorphUltralisk':[300,200,6,'ARMY'], 'MorphToOverseer':[50,50,0,'ARMY'], 'MorphMutalisk':[100,100,2,'ARMY'],    'MorphToLurker':[50,100,3,'ARMY'],\n    'MorphCorruptor':[150,100,2,'ARMY'], 'MorphViper':[100,200,3,'ARMY'],    'MorphToBroodLord':[150,150,4,'ARMY'], 'MorphToRavage':[25,75,3,'ARMY'],\n    \n    # UPGRADE\n    'EvolveFlyerAttacks1':[100,100,0,'UPGRADE'],             'EvolveFlyerAttacks2':[175,175,2,'UPGRADE'],             'EvolveFlyerAttacks3':[250,250,2,'UPGRADE'],\n    'EvolveFlyerCarapace1':[150,150,0,'UPGRADE'],            'EvolveFlyerCarapace2':[225,225,2,'UPGRADE'],            'EvolveFlyerCarapace3':[300,300,2,'UPGRADE'],\n    'EvolveBurrow':[100,100,0,'UPGRADE'],                    'EvolvePneumatizedCarapace':[100,100,0,'UPGRADE'],\n    'EvolvePathogenGlands':[150,150,0,'UPGRADE'],            'EvolveAdrenalGlands':[200,200,0,'UPGRADE'],             'EvolveMetabolicBoost':[100,100,0,'UPGRADE'],\n    'ResearchZergMeleeWeaponsLevel1':[100,100,0,'UPGRADE'],  'ResearchZergMeleeWeaponsLevel2':[150,150,0,'UPGRADE'],  'ResearchZergMeleeWeaponsLevel3':[200,200,0,'UPGRADE'],\n    'ResearchZergMissileWeaponsLevel1':[100,100,0,'UPGRADE'],'ResearchZergMissileWeaponsLevel2':[150,150,0,'UPGRADE'],'ResearchZergMissileWeaponsLevel3':[200,200,0,'UPGRADE'],\n    'ResearchZergGroundArmorsLevel1':[150,150,0,'UPGRADE'],  'ResearchZergGroundArmorsLevel2':[225,225,0,'UPGRADE'],  'ResearchZergGroundArmorsLevel3':[300,300,0,'UPGRADE'],\n    'EvolveTunnelingClaws':[100,100,0,'UPGRADE'],            'EvolveGlialReconstitution':[100,100,0,'UPGRADE'],       'EvolveCentrifugalHooks':[150,150,0,'UPGRADE'],\n    'ResearchEvolveMuscularAugments':[100,100,0,'UPGRADE'],  'ResearchAdaptiveTalons':[150,150,0,'UPGRADE'],          'ResearchSeismicSpines':[150,150,0,'UPGRADE'], \n    'EvolveNeuralParasite':[150,150,0,'UPGRADE'],            'EvolveChitinousPlating':[150,150,0,'UPGRADE'],          'EvolveAnabolicSynthesis':[150,150,0,'UPGRADE']\n            \n    }\n    \n    return Z_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature engineering <a id=\"5\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def feature_engineering(player_info, player_num, species, play_time, event_names, event_count, player_sight):\n    cols = ['VESPENE', 'SUPPLY', 'WORKER', 'BASE', 'BUILDING', 'DEFENSE', 'ARMY', 'UPGRADE', 'Minerals', 'Gas', 'Supply', 'Control_key']\n    df = pd.DataFrame({col:[0] for col in cols})\n    \n    ability_check = player_info[player_info['event'] == 'Ability']\n    onehot_species = pd.get_dummies(pd.Series(list('TPZ')))\n    \n    if species == 'T':\n        onehot_species = onehot_species.loc[onehot_species['T'] == 1].reset_index(drop=True)\n        species_dict = T_info()\n        idx = 0\n    elif species == 'P':\n        onehot_species = onehot_species.loc[onehot_species['P'] == 1].reset_index(drop=True)\n        species_dict = P_info()\n        idx = 1\n    elif species == 'Z':   \n        onehot_species = onehot_species.loc[onehot_species['Z'] == 1].reset_index(drop=True)\n        species_dict = Z_info()\n        idx = 2\n        \n    ##################### Add Basic Feature #####################\n    cols = list(event_count.columns)  \n    for col in event_names:\n        if col not in cols:\n            df[col] = pd.Series(0)\n        else:\n            df[col] = event_count[col]\n         \n    ##################### Add New Feature #####################\n    p = re.compile(\"- (\\w+)\", re.I)\n    for i in range(len(ability_check)):\n        match = p.findall(ability_check['event_contents'].iloc[i])\n        if match: \n            key = match[0]\n            if key in species_dict:\n                categories = species_dict[key][3]\n                df[categories] += 1\n                df['Minerals'] += species_dict[key][0]\n                df['Gas'] += species_dict[key][1] \n                df['Supply'] += species_dict[key][2]\n        else:\n            pass\n\n    ### Check Control keys ###\n    control_keys = ['Attack', 'Stop', 'Patrol', 'Move', 'HoldPosition', 'Rally', 'Gather', 'ReturnCargo', 'HaltBuilding', 'Cancel', 'Repair']\n    for control_key in control_keys:\n        cnt = len(ability_check[ability_check['event_contents'].str.contains(control_key, regex=True) == True])\n        if cnt > 0:\n            df['Control_key'] += cnt\n         \n    df['Micro'] = df['AddToControlGroup'] + df['GetControlGroup'] + df['SetControlGroup'] + df['ControlGroup'] + df['Control_key']\n    df['Macro'] = df['VESPENE'] + df['SUPPLY'] + df['WORKER'] + df['BASE'] + df['BUILDING'] + df['DEFENSE'] + df['ARMY'] + df['UPGRADE']\n    df['APM'] = df['Selection'] + df['Ability'] + df['Right Click'] + df['Micro']\n    df['Player_sight'] = pd.Series(player_sight)\n    df['Resource'] = df['Minerals'] + df['Gas']\n    df['Play_time'] = pd.Series(play_time)\n\n    cols = df.columns\n    df = pd.concat([df, onehot_species], axis=1)\n    df = df.rename(columns=lambda x:x+'_'+str(player_num))\n    \n    return df, cols","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 게임 유저의 맵의 이동 경로 활용\n\ndef transform_coordinate(data, play_time, t_interval=1, visualize=False):\n\n    # Assume the maximum size of the map is 200.\n    player_map = np.zeros([200,200,3], np.float)\n    filter_size = (20,40)\n    \n    for n in range(0, play_time, t_interval):\n        #display(data[(n <= data['time']) & (data['time'] <= n+t_interval)])\n        sample = data[(n <= data['time']) & (data['time'] <= n+t_interval)]\n        \n        # Camera\n        Cam = sample[sample['event'] == 'Camera']['event_contents'].str.extract(r'((\\d+).(\\d+)), ((\\d+).(\\d+))')\n        Cam_x = list(map(float, Cam[0].dropna().values))\n        Cam_y = list(map(float, Cam[3].dropna().values))\n        \n        # Right click\n        Right_click = sample[sample['event'] == 'Right Click']['event_contents'].str.extract(r'((\\d+).(\\d+)), ((\\d+).(\\d+))')\n        Right_click_x = list(map(float, Right_click[0].dropna().values))\n        Right_click_y = list(map(float, Right_click[3].dropna().values))\n\n        # Ability\n        Ability = sample[sample['event'] == 'Ability']\n\n        Build = Ability[Ability['event_contents'].str.contains('Build',regex=True) == True]['event_contents'].str.extract(r'((\\d+).(\\d+)), ((\\d+).(\\d+))')\n        Build_x = list(map(float, Build[0].dropna().values))\n        Build_y = list(map(float, Build[3].dropna().values))\n\n        Attack = Ability[Ability['event_contents'].str.contains('Attack',regex=True) == True]['event_contents'].str.extract(r'((\\d+).(\\d+)), ((\\d+).(\\d+))')\n        Attack_x = list(map(float, Attack[0].dropna().values))\n        Attack_y = list(map(float, Attack[3].dropna().values))\n        \n        Patrol = Ability[Ability['event_contents'].str.contains('Patrol',regex=True) == True]['event_contents'].str.extract(r'((\\d+).(\\d+)), ((\\d+).(\\d+))')\n        Patrol_x = list(map(float, Patrol[0].dropna().values))\n        Patrol_y = list(map(float, Patrol[3].dropna().values))\n        \n        for x, y in zip(Cam_x, Cam_y):\n            player_map[int(y)][int(x)] += 1\n        \n        for x, y in zip(Right_click_x, Right_click_y):\n            player_map[int(y)][int(x)] += 1\n      \n        if visualize:\n            labels = ['Cam', 'Right_click']\n            sns.scatterplot(x=Cam_x,         y=Cam_y)\n            sns.scatterplot(x=Right_click_x, y=Right_click_y)\n            plt.xlim(0,200)\n            plt.ylim(0,200)\n            plt.legend(labels)\n            plt.show()\n    \n    if visualize:\n        player_map = cv2.flip(player_map, 0)\n        player_map = cv2.dilate(player_map, np.ones(filter_size))\n        # G: 2,0 / R: 2,1 / B: 1,0\n        player_map[:,:,2] /= 255\n        player_map[:,:,1] /= 255\n        player_map[:,:,0] /= 255\n        plt.imshow(player_map)\n        plt.show()\n    \n    player_sight = sum(player_map.flatten())\n    \n    return player_map, player_sight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Preparation (Data Cleansing + Feature Engineering) <a id=\"6\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def data_preparation(data, split_time, testset=False):\n    event_names = ['Ability', 'AddToControlGroup', 'Camera', 'ControlGroup', 'GetControlGroup', \n                   'Right Click', 'Selection', 'SetControlGroup']\n        \n    g = data.groupby(['game_id', 'player'])\n    event_counts = g.event.value_counts()\n    m_time = g.time.max()\n    \n    if testset == False:\n        winners = g.winner.unique()\n    \n    x_data = pd.DataFrame()\n    y_data = pd.DataFrame()\n    gameIds = data['game_id'].unique()\n    \n    gc.collect()\n    \n    for gameId in tqdm(gameIds):\n        play_time = m_time[gameId].max()\n        \n        if check_missing_values(m_time[gameId]):\n            continue\n            \n        if testset == False:    \n            winner = pd.Series(winners[gameId,0])\n            \n        df = pd.DataFrame()\n        for player_num in range(2):\n            player_info = g.get_group((gameId, player_num))\n            species = player_info.species.unique()[0]\n            play_time = player_info.time.max()\n            event_count = event_counts[gameId, player_num].to_frame().T.reset_index(drop=True)\n            \n            _, player_sight = transform_coordinate(player_info, int(play_time)+1, split_time, visualize=False)\n            processed_data, cols = feature_engineering(player_info, player_num, species, play_time, event_names, event_count, player_sight)\n            \n            df = pd.concat([df, processed_data], axis=1)\n        \n        for col in cols:\n            df['delta_' + col] = df[col + '_1'][0] - df[col + '_0'][0]\n        \n        df['game_id'] = pd.Series(gameId)\n        x_data = pd.concat([x_data, df], axis=0)\n        \n        if testset == False:    \n            y_data = pd.concat([y_data, winner], axis=0)\n            \n        gc.collect()\n\n    x_data = x_data.set_index('game_id')\n    display(x_data.head(5))   \n    display(y_data.head(5))    \n\n    return x_data, y_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data Cleansing + Feature Engineering 완료된 데이터를 저장해둠\n\n'''\nsplit_time = 100\nx_data, y_data = data_preparation(test, split_time, testset=True)\n\ntrain_data = x_data.reset_index()\ntrain_data['winner'] = y_data.values\ntrain_data.to_feather('./processed_train2.ftr')\n\ntest_data = x_data.reset_index()\ntest_data.to_feather('./processed_test2.ftr')\n'''","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Data Partitioning <a id=\"7\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"x_data = train.drop(['game_id', 'winner'], axis=1)\ny_data = train['winner']\ntrain_x, val_x, train_y, val_y = train_test_split(x_data, y_data, test_size=0.2, random_state=0)\nprint(train_x.shape, train_y.shape, val_y.shape, val_x.shape)\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling - light Gradient Boosting Machine (LGBM) <a id=\"8\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nparams = {\n        'objective':'regression',\n        'metric':'auc',\n        'learning_rate':0.01,\n        'n_estimators': 500,\n}\n\nmodel = LGBMRegressor(**params)\nmodel.fit(\n    train_x, train_y,\n    eval_set=[(val_x, val_y)],\n    eval_metric='auc',\n    verbose=100,\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Importance <a id=\"9\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_importance = pd.DataFrame(sorted(zip(model.feature_importances_, train_x.columns)), columns=['Value','Feature'])\n\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_importance.sort_values(by=\"Value\", ascending=False)[:30])\nplt.title('LightGBM Features')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dendrogram \n\n# Keep only significant features\nto_keep = feature_importance.sort_values(by='Value', ascending=False)[:50].Feature\n\n## Create a Dendrogram to view highly correlated features\ncorr = np.round(scipy.stats.spearmanr(train_x[to_keep]).correlation, 4)\ncorr_condensed = hc.distance.squareform(1-corr)\nz = hc.linkage(corr_condensed, method='average')\nfig = plt.figure(figsize=(14,20))\ndendrogram = hc.dendrogram(z, labels=train_x[to_keep].columns, orientation='left', leaf_font_size=16)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(model, random_state=42).fit(val_x, val_y)\neli5.show_weights(perm, feature_names=list(val_x.columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import shap\nshap.initjs()\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(val_x)\nshap.summary_plot(shap_values, val_x, feature_names=list(val_x.columns))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature selection <a id=\"10\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"use_num_features = 0 # use_num_features = 0 인 경우 모든 컬럼의 데이터 사용\n\nscaler = StandardScaler()\nif use_num_features:\n    im_features = feature_importance.sort_values(by='Value', ascending=False)[:use_num_features].Feature \n    X_train = scaler.fit_transform(train_x[im_features].astype(np.float32))\n    Y_train = train_y.values\n    X_val = scaler.fit_transform(val_x[im_features].astype(np.float32))\n    Y_val = val_y.values\nelse:\n    X_train = scaler.fit_transform(train_x.astype(np.float32))\n    Y_train = train_y.values\n    X_val = scaler.fit_transform(val_x.astype(np.float32))\n    Y_val = val_y.values","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling - Multi Layer Perceptrons (MLP) <a id=\"11\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n    ''' Wrapper function to create a LearningRateScheduler with step decay schedule. '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch/step_size))\n    \n    return tf.keras.callbacks.LearningRateScheduler(schedule, verbose)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_model():\n    penalties = 0.01\n    stddev = 0.05\n\n    hidden_layer = tf.keras.layers.GaussianNoise(stddev)(inputs)\n    \n    hidden_layer = tf.keras.layers.Dense(128, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(penalties))(hidden_layer)\n    hidden_layer = tf.keras.activations.elu(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    hidden_layer = tf.keras.layers.GaussianNoise(stddev)(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(64, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(penalties))(hidden_layer)\n    hidden_layer = tf.keras.activations.elu(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    hidden_layer = tf.keras.layers.GaussianNoise(stddev)(hidden_layer)\n    \n    hidden_layer = tf.keras.layers.Dense(32, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(penalties))(hidden_layer)\n    hidden_layer = tf.keras.activations.elu(hidden_layer)\n    hidden_layer = tf.keras.layers.BatchNormalization()(hidden_layer)\n    hidden_layer = tf.keras.layers.GaussianNoise(stddev)(hidden_layer)\n\n    outputs = tf.keras.layers.Dense(1, kernel_initializer='normal', activation='sigmoid')(hidden_layer) \n    \n    return outputs","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(0)\ntf.random.set_seed(0)\n\nepochs = 1000\nlr = 0.01\nbatch_size = 8  * strategy.num_replicas_in_sync * 16 * 4\n\nsteps = len(X_train) // batch_size\n\noptimizer = tf.keras.optimizers.Adam()\n\nlr_sched = step_decay_schedule(initial_lr=lr, decay_factor=0.9, step_size=10, verbose=0)\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', patience=400, verbose=1)\n\ncallbacks_list = [lr_sched, early_stopping]\n\nwith strategy.scope():\n    inputs = tf.keras.Input(shape=(X_train.shape[1],))\n    output_lst = []\n    \n    outputs = get_model()\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n\n    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[tf.keras.metrics.AUC()])\n\n    history = model.fit(\n            X_train.astype(np.float32), Y_train.astype(np.float32),\n            shuffle=True,\n            epochs=epochs,\n            batch_size=batch_size,\n            validation_data = (X_val.astype(np.float32), Y_val.astype(np.float32)),\n            #validation_split=0.15,\n            steps_per_epoch=steps,\n            callbacks=callbacks_list,\n            verbose=0)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performance evaluation of MLP <a id=\"12\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"his = list(history.history)\nprint(\"Max AUC score (Validation dataset): \", max(history.history[his[3]]))\n\nplt.figure(figsize=(7,5))\nplt.plot(history.history[his[0]])\nplt.plot(history.history[his[2]])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\nplt.figure(figsize=(7,5))\nplt.plot(history.history[his[1]])\nplt.plot(history.history[his[3]])\nplt.title('AUC')\nplt.ylabel('AUC')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Submit Test Results <a id=\"13\"></a>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 한쪽 선수의 플레이 기록이 없는 경우, 플레이 기록이 있는 선수가 승리한 것으로 후처리 \ndef delete_missing_values(data): \n    g = data.groupby(['game_id', 'player'])\n    m_time = g.time.max()\n   \n    df = pd.DataFrame()\n    gameIds = data['game_id'].unique()\n    \n    id_lst = []\n    winner_lst = []\n    \n    for gameId in tqdm(gameIds):\n        if (m_time[gameId].min() <= 0) or (m_time[gameId].max() <= 0): \n            player_time = m_time[gameId]\n            id_lst.append(gameId)\n            winner_lst.append(np.where(player_time[0] >= player_time[1], 0, 1))\n            \n    df['game_id'] = id_lst\n    df['winner'] = winner_lst\n    \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"origin_test = pd.read_feather(\"/kaggle/input/star2-dataset/test.ftr\")\norigin_test = reduce_mem_usage(origin_test)\ntest_df = delete_missing_values(origin_test)\n\ntest = pd.read_feather(\"/kaggle/input/star2-processed-dataset/processed_test2.ftr\")\ntest = reduce_mem_usage(test)\ndisplay(test.head())\n\nX_test = test.drop(['game_id'], axis=1)\nX_test = scaler.fit_transform(X_test.astype(np.float32))\n\n# Predict using DNN\npred = model.predict(X_test.astype(np.float32))\nprint('Prediction values range: {0} ~ {1}'.format(pred.min(), pred.max()))\n\ntest['winner'] = pred\ntest_result = test[['game_id', 'winner']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = test_result.append(test_df, ignore_index=True).sort_values(by='game_id')\nsubmission.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Last check of submission\nprint('Head of submission: ')\ndisplay(submission.head())","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}